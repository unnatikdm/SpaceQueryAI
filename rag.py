# -*- coding: utf-8 -*-
"""rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Ra7nxEVWWwccr10OJcGaiI_-Z3V-Mvd
"""

# -*- coding: utf-8 -*-
"""Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

!pip install sentence-transformers faiss-cpu

from google.colab import files
uploaded = files.upload()

import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# === Load Extracted Text ===
with open("mosdac_html_text_data.json", "r", encoding="utf-8") as f:
    html_data = json.load(f)

# === Chunking ===
chunk_size = 750
chunks = []
for entry in html_data:
    words = entry["text"].split()
    for i in range(0, len(words), chunk_size):
        chunk_text = " ".join(words[i:i + chunk_size])
        chunks.append({
            "text": chunk_text,
            "source": entry["file"]
        })

print("Chunks:", len(chunks))

# === Embeddings ===
model = SentenceTransformer("all-MiniLM-L6-v2")
texts = [chunk["text"] for chunk in chunks]
embeddings = model.encode(texts, show_progress_bar=True)

# === FAISS Index ===
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(np.array(embeddings))
print("Index ready.")

# === Semantic Search Function ===
def ask(query, top_k=5):
    q_emb = model.encode([query])
    D, I = index.search(np.array(q_emb), top_k)
    return [{"text": chunks[i]["text"], "source": chunks[i]["source"]} for i in I[0]]

# === Example Use ===
query = "What is the role of INSAT-3DR?"
results = ask(query)
for i, res in enumerate(results, 1):
    print(f"\n--- Result {i} (From: {res['source']}) ---")
    print(res["text"][:500], "...")

!pip install streamlit
!streamlit run app.py

!pip install gradio sentence-transformers faiss-cpu

import json
import faiss
import numpy as np
import gradio as gr
from sentence_transformers import SentenceTransformer

# === Load Extracted Text ===
with open("mosdac_html_text_data.json", "r", encoding="utf-8") as f:
    html_data = json.load(f)

# === Chunking ===
chunk_size = 750
chunks = []
for entry in html_data:
    words = entry["text"].split()
    for i in range(0, len(words), chunk_size):
        chunk_text = " ".join(words[i:i + chunk_size])
        chunks.append({"text": chunk_text, "source": entry["file"]})

# === Embedding Model ===
model = SentenceTransformer("all-MiniLM-L6-v2")
texts = [chunk["text"] for chunk in chunks]
embeddings = model.encode(texts, show_progress_bar=True)

# === FAISS Index ===
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(np.array(embeddings))

# === Query Function ===
def query_rag(user_input, top_k=3):
    q_emb = model.encode([user_input])
    D, I = index.search(np.array(q_emb), top_k)
    results = []
    for i in I[0]:
        source = chunks[i]["source"]
        preview = chunks[i]["text"][:1000] + ("..." if len(chunks[i]["text"]) > 1000 else "")
        results.append(f"üìÑ **Source:** `{source}`\n\n{preview}")
    return "\n\n---\n\n".join(results)

# === Gradio Interface ===
interface = gr.Interface(
    fn=query_rag,
    inputs=gr.Textbox(lines=2, placeholder="Ask about ISRO or MOSDAC data..."),
    outputs="markdown",
    title="üîç ISRO Help Bot (MOSDAC RAG)",
    description="Ask questions based on data crawled from the MOSDAC portal."
)

interface.launch()

!pip install transformers accelerate bitsandbytes

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"  # You can also use "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)